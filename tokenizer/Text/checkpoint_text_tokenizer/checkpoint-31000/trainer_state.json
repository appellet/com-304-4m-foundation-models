{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.932713873758411,
  "eval_steps": 500,
  "global_step": 31000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03204101249599487,
      "grad_norm": 21.766746520996094,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 8.3805,
      "step": 100
    },
    {
      "epoch": 0.06408202499198974,
      "grad_norm": 25.362688064575195,
      "learning_rate": 4.975e-05,
      "loss": 6.2795,
      "step": 200
    },
    {
      "epoch": 0.09612303748798462,
      "grad_norm": 21.9725341796875,
      "learning_rate": 4.984037407287972e-05,
      "loss": 5.9518,
      "step": 300
    },
    {
      "epoch": 0.12816404998397948,
      "grad_norm": 21.030479431152344,
      "learning_rate": 4.967913576265721e-05,
      "loss": 5.8538,
      "step": 400
    },
    {
      "epoch": 0.16020506247997437,
      "grad_norm": 16.171945571899414,
      "learning_rate": 4.95178974524347e-05,
      "loss": 5.7317,
      "step": 500
    },
    {
      "epoch": 0.19224607497596924,
      "grad_norm": 18.56962776184082,
      "learning_rate": 4.935665914221219e-05,
      "loss": 5.6358,
      "step": 600
    },
    {
      "epoch": 0.2242870874719641,
      "grad_norm": 13.1920166015625,
      "learning_rate": 4.9195420831989684e-05,
      "loss": 5.5436,
      "step": 700
    },
    {
      "epoch": 0.25632809996795897,
      "grad_norm": 11.890334129333496,
      "learning_rate": 4.9034182521767176e-05,
      "loss": 5.4937,
      "step": 800
    },
    {
      "epoch": 0.28836911246395386,
      "grad_norm": 11.79752254486084,
      "learning_rate": 4.887294421154466e-05,
      "loss": 5.4619,
      "step": 900
    },
    {
      "epoch": 0.32041012495994875,
      "grad_norm": 9.47669506072998,
      "learning_rate": 4.871170590132216e-05,
      "loss": 5.3286,
      "step": 1000
    },
    {
      "epoch": 0.3524511374559436,
      "grad_norm": 8.227592468261719,
      "learning_rate": 4.8550467591099644e-05,
      "loss": 5.3943,
      "step": 1100
    },
    {
      "epoch": 0.3844921499519385,
      "grad_norm": 10.64306354522705,
      "learning_rate": 4.838922928087714e-05,
      "loss": 5.2491,
      "step": 1200
    },
    {
      "epoch": 0.41653316244793337,
      "grad_norm": 8.853401184082031,
      "learning_rate": 4.822799097065463e-05,
      "loss": 5.2594,
      "step": 1300
    },
    {
      "epoch": 0.4485741749439282,
      "grad_norm": 7.6145710945129395,
      "learning_rate": 4.806675266043212e-05,
      "loss": 5.2191,
      "step": 1400
    },
    {
      "epoch": 0.4806151874399231,
      "grad_norm": 7.532911777496338,
      "learning_rate": 4.790551435020961e-05,
      "loss": 5.1062,
      "step": 1500
    },
    {
      "epoch": 0.5126561999359179,
      "grad_norm": 5.226959705352783,
      "learning_rate": 4.77442760399871e-05,
      "loss": 5.2446,
      "step": 1600
    },
    {
      "epoch": 0.5446972124319128,
      "grad_norm": 7.724085330963135,
      "learning_rate": 4.758303772976459e-05,
      "loss": 5.1156,
      "step": 1700
    },
    {
      "epoch": 0.5767382249279077,
      "grad_norm": 5.596065044403076,
      "learning_rate": 4.7421799419542084e-05,
      "loss": 5.0797,
      "step": 1800
    },
    {
      "epoch": 0.6087792374239026,
      "grad_norm": 6.382579326629639,
      "learning_rate": 4.7260561109319576e-05,
      "loss": 5.0947,
      "step": 1900
    },
    {
      "epoch": 0.6408202499198975,
      "grad_norm": 5.1149187088012695,
      "learning_rate": 4.709932279909707e-05,
      "loss": 5.0573,
      "step": 2000
    },
    {
      "epoch": 0.6728612624158924,
      "grad_norm": 6.5251874923706055,
      "learning_rate": 4.693808448887456e-05,
      "loss": 5.0591,
      "step": 2100
    },
    {
      "epoch": 0.7049022749118872,
      "grad_norm": 6.306258201599121,
      "learning_rate": 4.677684617865205e-05,
      "loss": 4.9593,
      "step": 2200
    },
    {
      "epoch": 0.7369432874078821,
      "grad_norm": 6.696471691131592,
      "learning_rate": 4.661560786842954e-05,
      "loss": 5.0148,
      "step": 2300
    },
    {
      "epoch": 0.768984299903877,
      "grad_norm": 5.74586820602417,
      "learning_rate": 4.6454369558207034e-05,
      "loss": 4.9653,
      "step": 2400
    },
    {
      "epoch": 0.8010253123998718,
      "grad_norm": 4.710788726806641,
      "learning_rate": 4.6293131247984525e-05,
      "loss": 4.958,
      "step": 2500
    },
    {
      "epoch": 0.8330663248958667,
      "grad_norm": 5.486098766326904,
      "learning_rate": 4.613189293776202e-05,
      "loss": 4.8868,
      "step": 2600
    },
    {
      "epoch": 0.8651073373918616,
      "grad_norm": 5.552064418792725,
      "learning_rate": 4.597065462753951e-05,
      "loss": 4.8952,
      "step": 2700
    },
    {
      "epoch": 0.8971483498878564,
      "grad_norm": 4.634886741638184,
      "learning_rate": 4.580941631731699e-05,
      "loss": 4.8638,
      "step": 2800
    },
    {
      "epoch": 0.9291893623838513,
      "grad_norm": 5.468979835510254,
      "learning_rate": 4.564817800709449e-05,
      "loss": 4.8361,
      "step": 2900
    },
    {
      "epoch": 0.9612303748798462,
      "grad_norm": 5.579902172088623,
      "learning_rate": 4.5486939696871976e-05,
      "loss": 4.8612,
      "step": 3000
    },
    {
      "epoch": 0.9932713873758411,
      "grad_norm": 4.941598415374756,
      "learning_rate": 4.5325701386649475e-05,
      "loss": 4.8029,
      "step": 3100
    },
    {
      "epoch": 1.0253123998718359,
      "grad_norm": 4.899550914764404,
      "learning_rate": 4.516446307642696e-05,
      "loss": 4.7369,
      "step": 3200
    },
    {
      "epoch": 1.0573534123678308,
      "grad_norm": 6.027024269104004,
      "learning_rate": 4.500322476620446e-05,
      "loss": 4.6826,
      "step": 3300
    },
    {
      "epoch": 1.0893944248638257,
      "grad_norm": 4.829816818237305,
      "learning_rate": 4.484198645598194e-05,
      "loss": 4.7022,
      "step": 3400
    },
    {
      "epoch": 1.1214354373598205,
      "grad_norm": 4.983593463897705,
      "learning_rate": 4.4680748145759434e-05,
      "loss": 4.7215,
      "step": 3500
    },
    {
      "epoch": 1.1534764498558154,
      "grad_norm": 4.734386920928955,
      "learning_rate": 4.4519509835536925e-05,
      "loss": 4.6595,
      "step": 3600
    },
    {
      "epoch": 1.1855174623518103,
      "grad_norm": 5.9595208168029785,
      "learning_rate": 4.435827152531442e-05,
      "loss": 4.65,
      "step": 3700
    },
    {
      "epoch": 1.2175584748478052,
      "grad_norm": 5.356624126434326,
      "learning_rate": 4.419703321509191e-05,
      "loss": 4.6288,
      "step": 3800
    },
    {
      "epoch": 1.2495994873438,
      "grad_norm": 5.094172954559326,
      "learning_rate": 4.40357949048694e-05,
      "loss": 4.6572,
      "step": 3900
    },
    {
      "epoch": 1.281640499839795,
      "grad_norm": 6.770674705505371,
      "learning_rate": 4.387455659464689e-05,
      "loss": 4.6468,
      "step": 4000
    },
    {
      "epoch": 1.31368151233579,
      "grad_norm": 5.556169033050537,
      "learning_rate": 4.371331828442438e-05,
      "loss": 4.5721,
      "step": 4100
    },
    {
      "epoch": 1.3457225248317846,
      "grad_norm": 5.833667755126953,
      "learning_rate": 4.355207997420187e-05,
      "loss": 4.596,
      "step": 4200
    },
    {
      "epoch": 1.3777635373277795,
      "grad_norm": 5.067713260650635,
      "learning_rate": 4.3390841663979366e-05,
      "loss": 4.5883,
      "step": 4300
    },
    {
      "epoch": 1.4098045498237743,
      "grad_norm": 6.050638198852539,
      "learning_rate": 4.322960335375685e-05,
      "loss": 4.5705,
      "step": 4400
    },
    {
      "epoch": 1.4418455623197692,
      "grad_norm": 5.583294868469238,
      "learning_rate": 4.306836504353435e-05,
      "loss": 4.611,
      "step": 4500
    },
    {
      "epoch": 1.4738865748157641,
      "grad_norm": 4.766455173492432,
      "learning_rate": 4.2907126733311834e-05,
      "loss": 4.5798,
      "step": 4600
    },
    {
      "epoch": 1.505927587311759,
      "grad_norm": 5.801910400390625,
      "learning_rate": 4.274588842308933e-05,
      "loss": 4.6178,
      "step": 4700
    },
    {
      "epoch": 1.537968599807754,
      "grad_norm": 6.825872421264648,
      "learning_rate": 4.258465011286682e-05,
      "loss": 4.6132,
      "step": 4800
    },
    {
      "epoch": 1.5700096123037488,
      "grad_norm": 5.248273849487305,
      "learning_rate": 4.242341180264431e-05,
      "loss": 4.5965,
      "step": 4900
    },
    {
      "epoch": 1.6020506247997437,
      "grad_norm": 4.532475471496582,
      "learning_rate": 4.22621734924218e-05,
      "loss": 4.5621,
      "step": 5000
    },
    {
      "epoch": 1.6340916372957386,
      "grad_norm": 5.167999744415283,
      "learning_rate": 4.210093518219929e-05,
      "loss": 4.5447,
      "step": 5100
    },
    {
      "epoch": 1.6661326497917335,
      "grad_norm": 5.231969356536865,
      "learning_rate": 4.1939696871976783e-05,
      "loss": 4.5533,
      "step": 5200
    },
    {
      "epoch": 1.6981736622877284,
      "grad_norm": 3.955052137374878,
      "learning_rate": 4.1778458561754275e-05,
      "loss": 4.572,
      "step": 5300
    },
    {
      "epoch": 1.7302146747837233,
      "grad_norm": 7.179714679718018,
      "learning_rate": 4.1617220251531767e-05,
      "loss": 4.542,
      "step": 5400
    },
    {
      "epoch": 1.7622556872797182,
      "grad_norm": 6.284963130950928,
      "learning_rate": 4.145598194130926e-05,
      "loss": 4.5383,
      "step": 5500
    },
    {
      "epoch": 1.794296699775713,
      "grad_norm": 6.358280658721924,
      "learning_rate": 4.129474363108675e-05,
      "loss": 4.5311,
      "step": 5600
    },
    {
      "epoch": 1.826337712271708,
      "grad_norm": 5.637439250946045,
      "learning_rate": 4.113350532086424e-05,
      "loss": 4.5188,
      "step": 5700
    },
    {
      "epoch": 1.8583787247677026,
      "grad_norm": 5.372714996337891,
      "learning_rate": 4.097226701064173e-05,
      "loss": 4.5568,
      "step": 5800
    },
    {
      "epoch": 1.8904197372636975,
      "grad_norm": 4.441945552825928,
      "learning_rate": 4.0811028700419224e-05,
      "loss": 4.5204,
      "step": 5900
    },
    {
      "epoch": 1.9224607497596924,
      "grad_norm": 4.4015583992004395,
      "learning_rate": 4.0649790390196716e-05,
      "loss": 4.4884,
      "step": 6000
    },
    {
      "epoch": 1.9545017622556873,
      "grad_norm": 5.894169807434082,
      "learning_rate": 4.04885520799742e-05,
      "loss": 4.5482,
      "step": 6100
    },
    {
      "epoch": 1.9865427747516822,
      "grad_norm": 5.3107404708862305,
      "learning_rate": 4.03273137697517e-05,
      "loss": 4.5473,
      "step": 6200
    },
    {
      "epoch": 2.018583787247677,
      "grad_norm": 7.629248142242432,
      "learning_rate": 4.0166075459529184e-05,
      "loss": 4.4343,
      "step": 6300
    },
    {
      "epoch": 2.0506247997436717,
      "grad_norm": 6.344351291656494,
      "learning_rate": 4.000483714930668e-05,
      "loss": 4.3595,
      "step": 6400
    },
    {
      "epoch": 2.0826658122396666,
      "grad_norm": 5.590726375579834,
      "learning_rate": 3.984359883908417e-05,
      "loss": 4.3811,
      "step": 6500
    },
    {
      "epoch": 2.1147068247356615,
      "grad_norm": 5.9778265953063965,
      "learning_rate": 3.9682360528861665e-05,
      "loss": 4.3397,
      "step": 6600
    },
    {
      "epoch": 2.1467478372316564,
      "grad_norm": 5.356808662414551,
      "learning_rate": 3.952112221863915e-05,
      "loss": 4.3016,
      "step": 6700
    },
    {
      "epoch": 2.1787888497276513,
      "grad_norm": 5.44002628326416,
      "learning_rate": 3.935988390841664e-05,
      "loss": 4.4674,
      "step": 6800
    },
    {
      "epoch": 2.210829862223646,
      "grad_norm": 5.414182186126709,
      "learning_rate": 3.919864559819413e-05,
      "loss": 4.4221,
      "step": 6900
    },
    {
      "epoch": 2.242870874719641,
      "grad_norm": 5.768028736114502,
      "learning_rate": 3.9037407287971625e-05,
      "loss": 4.33,
      "step": 7000
    },
    {
      "epoch": 2.274911887215636,
      "grad_norm": 6.535171985626221,
      "learning_rate": 3.8876168977749116e-05,
      "loss": 4.323,
      "step": 7100
    },
    {
      "epoch": 2.306952899711631,
      "grad_norm": 6.486462593078613,
      "learning_rate": 3.871493066752661e-05,
      "loss": 4.3972,
      "step": 7200
    },
    {
      "epoch": 2.3389939122076258,
      "grad_norm": 5.911732196807861,
      "learning_rate": 3.85536923573041e-05,
      "loss": 4.3447,
      "step": 7300
    },
    {
      "epoch": 2.3710349247036206,
      "grad_norm": 6.483943939208984,
      "learning_rate": 3.839245404708159e-05,
      "loss": 4.3125,
      "step": 7400
    },
    {
      "epoch": 2.4030759371996155,
      "grad_norm": 6.265339374542236,
      "learning_rate": 3.8231215736859076e-05,
      "loss": 4.3615,
      "step": 7500
    },
    {
      "epoch": 2.4351169496956104,
      "grad_norm": 5.053999423980713,
      "learning_rate": 3.8069977426636574e-05,
      "loss": 4.3766,
      "step": 7600
    },
    {
      "epoch": 2.4671579621916053,
      "grad_norm": 6.382936000823975,
      "learning_rate": 3.790873911641406e-05,
      "loss": 4.3131,
      "step": 7700
    },
    {
      "epoch": 2.4991989746876,
      "grad_norm": 5.064751148223877,
      "learning_rate": 3.774750080619156e-05,
      "loss": 4.3701,
      "step": 7800
    },
    {
      "epoch": 2.531239987183595,
      "grad_norm": 4.4849853515625,
      "learning_rate": 3.758626249596904e-05,
      "loss": 4.3555,
      "step": 7900
    },
    {
      "epoch": 2.56328099967959,
      "grad_norm": 5.221761226654053,
      "learning_rate": 3.742502418574654e-05,
      "loss": 4.3876,
      "step": 8000
    },
    {
      "epoch": 2.595322012175585,
      "grad_norm": 5.012213230133057,
      "learning_rate": 3.7263785875524025e-05,
      "loss": 4.3312,
      "step": 8100
    },
    {
      "epoch": 2.62736302467158,
      "grad_norm": 5.337954521179199,
      "learning_rate": 3.7102547565301516e-05,
      "loss": 4.3368,
      "step": 8200
    },
    {
      "epoch": 2.6594040371675742,
      "grad_norm": 5.557770729064941,
      "learning_rate": 3.694130925507901e-05,
      "loss": 4.3035,
      "step": 8300
    },
    {
      "epoch": 2.691445049663569,
      "grad_norm": 6.429680824279785,
      "learning_rate": 3.67800709448565e-05,
      "loss": 4.3399,
      "step": 8400
    },
    {
      "epoch": 2.723486062159564,
      "grad_norm": 5.816980838775635,
      "learning_rate": 3.661883263463399e-05,
      "loss": 4.3701,
      "step": 8500
    },
    {
      "epoch": 2.755527074655559,
      "grad_norm": 7.148831844329834,
      "learning_rate": 3.645759432441148e-05,
      "loss": 4.3083,
      "step": 8600
    },
    {
      "epoch": 2.787568087151554,
      "grad_norm": 6.69079065322876,
      "learning_rate": 3.6296356014188974e-05,
      "loss": 4.3397,
      "step": 8700
    },
    {
      "epoch": 2.8196090996475487,
      "grad_norm": 6.015012264251709,
      "learning_rate": 3.6135117703966466e-05,
      "loss": 4.3249,
      "step": 8800
    },
    {
      "epoch": 2.8516501121435436,
      "grad_norm": 6.130766868591309,
      "learning_rate": 3.597387939374395e-05,
      "loss": 4.3053,
      "step": 8900
    },
    {
      "epoch": 2.8836911246395385,
      "grad_norm": 6.060104846954346,
      "learning_rate": 3.581264108352145e-05,
      "loss": 4.3613,
      "step": 9000
    },
    {
      "epoch": 2.9157321371355334,
      "grad_norm": 6.393442630767822,
      "learning_rate": 3.5651402773298934e-05,
      "loss": 4.2284,
      "step": 9100
    },
    {
      "epoch": 2.9477731496315283,
      "grad_norm": 5.359652042388916,
      "learning_rate": 3.549016446307643e-05,
      "loss": 4.296,
      "step": 9200
    },
    {
      "epoch": 2.979814162127523,
      "grad_norm": 7.310267925262451,
      "learning_rate": 3.532892615285392e-05,
      "loss": 4.283,
      "step": 9300
    },
    {
      "epoch": 3.011855174623518,
      "grad_norm": 6.980048656463623,
      "learning_rate": 3.516768784263141e-05,
      "loss": 4.2408,
      "step": 9400
    },
    {
      "epoch": 3.043896187119513,
      "grad_norm": 5.052781105041504,
      "learning_rate": 3.50064495324089e-05,
      "loss": 4.1813,
      "step": 9500
    },
    {
      "epoch": 3.075937199615508,
      "grad_norm": 5.047206401824951,
      "learning_rate": 3.484521122218639e-05,
      "loss": 4.0984,
      "step": 9600
    },
    {
      "epoch": 3.1079782121115027,
      "grad_norm": 7.678863525390625,
      "learning_rate": 3.468397291196388e-05,
      "loss": 4.1204,
      "step": 9700
    },
    {
      "epoch": 3.1400192246074976,
      "grad_norm": 5.432503700256348,
      "learning_rate": 3.4522734601741374e-05,
      "loss": 4.1731,
      "step": 9800
    },
    {
      "epoch": 3.1720602371034925,
      "grad_norm": 5.9262542724609375,
      "learning_rate": 3.4361496291518866e-05,
      "loss": 4.1706,
      "step": 9900
    },
    {
      "epoch": 3.2041012495994874,
      "grad_norm": 5.560184001922607,
      "learning_rate": 3.420025798129636e-05,
      "loss": 4.1494,
      "step": 10000
    },
    {
      "epoch": 3.2361422620954823,
      "grad_norm": 5.72817850112915,
      "learning_rate": 3.403901967107385e-05,
      "loss": 4.2373,
      "step": 10100
    },
    {
      "epoch": 3.268183274591477,
      "grad_norm": 6.1484808921813965,
      "learning_rate": 3.387778136085134e-05,
      "loss": 4.1646,
      "step": 10200
    },
    {
      "epoch": 3.300224287087472,
      "grad_norm": 5.478635787963867,
      "learning_rate": 3.371654305062883e-05,
      "loss": 4.1915,
      "step": 10300
    },
    {
      "epoch": 3.332265299583467,
      "grad_norm": 6.0843987464904785,
      "learning_rate": 3.3555304740406324e-05,
      "loss": 4.1654,
      "step": 10400
    },
    {
      "epoch": 3.364306312079462,
      "grad_norm": 6.795510292053223,
      "learning_rate": 3.3394066430183815e-05,
      "loss": 4.1464,
      "step": 10500
    },
    {
      "epoch": 3.3963473245754567,
      "grad_norm": 5.445790767669678,
      "learning_rate": 3.323282811996131e-05,
      "loss": 4.1437,
      "step": 10600
    },
    {
      "epoch": 3.4283883370714516,
      "grad_norm": 6.338263988494873,
      "learning_rate": 3.30715898097388e-05,
      "loss": 4.1917,
      "step": 10700
    },
    {
      "epoch": 3.4604293495674465,
      "grad_norm": 7.462223529815674,
      "learning_rate": 3.291035149951628e-05,
      "loss": 4.1144,
      "step": 10800
    },
    {
      "epoch": 3.4924703620634414,
      "grad_norm": 5.44978141784668,
      "learning_rate": 3.274911318929378e-05,
      "loss": 4.1744,
      "step": 10900
    },
    {
      "epoch": 3.5245113745594363,
      "grad_norm": 6.155714511871338,
      "learning_rate": 3.2587874879071266e-05,
      "loss": 4.2251,
      "step": 11000
    },
    {
      "epoch": 3.556552387055431,
      "grad_norm": 5.108409881591797,
      "learning_rate": 3.2426636568848764e-05,
      "loss": 4.1821,
      "step": 11100
    },
    {
      "epoch": 3.588593399551426,
      "grad_norm": 7.293722152709961,
      "learning_rate": 3.226539825862625e-05,
      "loss": 4.1625,
      "step": 11200
    },
    {
      "epoch": 3.6206344120474205,
      "grad_norm": 5.436421871185303,
      "learning_rate": 3.210415994840375e-05,
      "loss": 4.1328,
      "step": 11300
    },
    {
      "epoch": 3.6526754245434154,
      "grad_norm": 6.063522815704346,
      "learning_rate": 3.194292163818123e-05,
      "loss": 4.2266,
      "step": 11400
    },
    {
      "epoch": 3.6847164370394103,
      "grad_norm": 6.104968547821045,
      "learning_rate": 3.1781683327958724e-05,
      "loss": 4.0894,
      "step": 11500
    },
    {
      "epoch": 3.716757449535405,
      "grad_norm": 6.759021282196045,
      "learning_rate": 3.1620445017736215e-05,
      "loss": 4.1461,
      "step": 11600
    },
    {
      "epoch": 3.7487984620314,
      "grad_norm": 6.688625812530518,
      "learning_rate": 3.145920670751371e-05,
      "loss": 4.1523,
      "step": 11700
    },
    {
      "epoch": 3.780839474527395,
      "grad_norm": 7.041533470153809,
      "learning_rate": 3.12979683972912e-05,
      "loss": 4.175,
      "step": 11800
    },
    {
      "epoch": 3.81288048702339,
      "grad_norm": 6.239899158477783,
      "learning_rate": 3.113673008706869e-05,
      "loss": 4.1589,
      "step": 11900
    },
    {
      "epoch": 3.8449214995193848,
      "grad_norm": 5.816464424133301,
      "learning_rate": 3.097549177684618e-05,
      "loss": 4.1493,
      "step": 12000
    },
    {
      "epoch": 3.8769625120153797,
      "grad_norm": 5.323440074920654,
      "learning_rate": 3.081425346662367e-05,
      "loss": 4.1301,
      "step": 12100
    },
    {
      "epoch": 3.9090035245113746,
      "grad_norm": 5.185622692108154,
      "learning_rate": 3.065301515640116e-05,
      "loss": 4.133,
      "step": 12200
    },
    {
      "epoch": 3.9410445370073695,
      "grad_norm": 5.682469844818115,
      "learning_rate": 3.0491776846178656e-05,
      "loss": 4.1251,
      "step": 12300
    },
    {
      "epoch": 3.9730855495033643,
      "grad_norm": 5.670216083526611,
      "learning_rate": 3.0330538535956144e-05,
      "loss": 4.1666,
      "step": 12400
    },
    {
      "epoch": 4.005126561999359,
      "grad_norm": 6.5513505935668945,
      "learning_rate": 3.0169300225733636e-05,
      "loss": 4.1213,
      "step": 12500
    },
    {
      "epoch": 4.037167574495354,
      "grad_norm": 6.0540289878845215,
      "learning_rate": 3.0008061915511128e-05,
      "loss": 3.9852,
      "step": 12600
    },
    {
      "epoch": 4.069208586991349,
      "grad_norm": 6.845970630645752,
      "learning_rate": 2.984682360528862e-05,
      "loss": 3.9749,
      "step": 12700
    },
    {
      "epoch": 4.1012495994873435,
      "grad_norm": 7.065426349639893,
      "learning_rate": 2.968558529506611e-05,
      "loss": 4.0098,
      "step": 12800
    },
    {
      "epoch": 4.133290611983338,
      "grad_norm": 6.581762790679932,
      "learning_rate": 2.95243469848436e-05,
      "loss": 3.9731,
      "step": 12900
    },
    {
      "epoch": 4.165331624479333,
      "grad_norm": 6.075226783752441,
      "learning_rate": 2.9363108674621094e-05,
      "loss": 3.9845,
      "step": 13000
    },
    {
      "epoch": 4.197372636975328,
      "grad_norm": 5.817797660827637,
      "learning_rate": 2.9201870364398582e-05,
      "loss": 3.9894,
      "step": 13100
    },
    {
      "epoch": 4.229413649471323,
      "grad_norm": 6.677702903747559,
      "learning_rate": 2.9040632054176077e-05,
      "loss": 4.0398,
      "step": 13200
    },
    {
      "epoch": 4.261454661967318,
      "grad_norm": 6.790094375610352,
      "learning_rate": 2.8879393743953565e-05,
      "loss": 4.0132,
      "step": 13300
    },
    {
      "epoch": 4.293495674463313,
      "grad_norm": 6.238937854766846,
      "learning_rate": 2.8718155433731053e-05,
      "loss": 4.0119,
      "step": 13400
    },
    {
      "epoch": 4.325536686959308,
      "grad_norm": 9.537263870239258,
      "learning_rate": 2.8556917123508548e-05,
      "loss": 3.9858,
      "step": 13500
    },
    {
      "epoch": 4.357577699455303,
      "grad_norm": 6.103856086730957,
      "learning_rate": 2.8395678813286036e-05,
      "loss": 4.0389,
      "step": 13600
    },
    {
      "epoch": 4.3896187119512975,
      "grad_norm": 6.887875080108643,
      "learning_rate": 2.823444050306353e-05,
      "loss": 3.9694,
      "step": 13700
    },
    {
      "epoch": 4.421659724447292,
      "grad_norm": 6.493252277374268,
      "learning_rate": 2.807320219284102e-05,
      "loss": 3.9728,
      "step": 13800
    },
    {
      "epoch": 4.453700736943287,
      "grad_norm": 7.605747222900391,
      "learning_rate": 2.7911963882618514e-05,
      "loss": 3.9753,
      "step": 13900
    },
    {
      "epoch": 4.485741749439282,
      "grad_norm": 8.80678939819336,
      "learning_rate": 2.7750725572396002e-05,
      "loss": 4.0382,
      "step": 14000
    },
    {
      "epoch": 4.517782761935277,
      "grad_norm": 6.682621479034424,
      "learning_rate": 2.758948726217349e-05,
      "loss": 3.9778,
      "step": 14100
    },
    {
      "epoch": 4.549823774431272,
      "grad_norm": 6.919284820556641,
      "learning_rate": 2.7428248951950986e-05,
      "loss": 3.9962,
      "step": 14200
    },
    {
      "epoch": 4.581864786927267,
      "grad_norm": 8.002461433410645,
      "learning_rate": 2.7267010641728474e-05,
      "loss": 4.0673,
      "step": 14300
    },
    {
      "epoch": 4.613905799423262,
      "grad_norm": 5.877903461456299,
      "learning_rate": 2.710577233150597e-05,
      "loss": 4.0012,
      "step": 14400
    },
    {
      "epoch": 4.645946811919257,
      "grad_norm": 6.954878807067871,
      "learning_rate": 2.6944534021283457e-05,
      "loss": 4.0199,
      "step": 14500
    },
    {
      "epoch": 4.6779878244152515,
      "grad_norm": 6.425539493560791,
      "learning_rate": 2.6783295711060952e-05,
      "loss": 3.9694,
      "step": 14600
    },
    {
      "epoch": 4.710028836911246,
      "grad_norm": 9.810807228088379,
      "learning_rate": 2.662205740083844e-05,
      "loss": 4.0537,
      "step": 14700
    },
    {
      "epoch": 4.742069849407241,
      "grad_norm": 8.23226547241211,
      "learning_rate": 2.6460819090615928e-05,
      "loss": 4.0096,
      "step": 14800
    },
    {
      "epoch": 4.774110861903236,
      "grad_norm": 7.4733710289001465,
      "learning_rate": 2.6299580780393423e-05,
      "loss": 4.0014,
      "step": 14900
    },
    {
      "epoch": 4.806151874399231,
      "grad_norm": 6.174025058746338,
      "learning_rate": 2.613834247017091e-05,
      "loss": 4.0706,
      "step": 15000
    },
    {
      "epoch": 4.838192886895226,
      "grad_norm": 5.049755096435547,
      "learning_rate": 2.5977104159948406e-05,
      "loss": 4.0367,
      "step": 15100
    },
    {
      "epoch": 4.870233899391221,
      "grad_norm": 6.788947105407715,
      "learning_rate": 2.5815865849725894e-05,
      "loss": 4.031,
      "step": 15200
    },
    {
      "epoch": 4.902274911887216,
      "grad_norm": 8.53971004486084,
      "learning_rate": 2.565462753950339e-05,
      "loss": 4.0285,
      "step": 15300
    },
    {
      "epoch": 4.934315924383211,
      "grad_norm": 6.8103461265563965,
      "learning_rate": 2.5493389229280877e-05,
      "loss": 4.019,
      "step": 15400
    },
    {
      "epoch": 4.9663569368792055,
      "grad_norm": 6.808667182922363,
      "learning_rate": 2.533215091905837e-05,
      "loss": 4.0168,
      "step": 15500
    },
    {
      "epoch": 4.9983979493752,
      "grad_norm": 6.377023696899414,
      "learning_rate": 2.517091260883586e-05,
      "loss": 3.9894,
      "step": 15600
    },
    {
      "epoch": 5.030438961871195,
      "grad_norm": 6.519725799560547,
      "learning_rate": 2.5009674298613352e-05,
      "loss": 3.888,
      "step": 15700
    },
    {
      "epoch": 5.06247997436719,
      "grad_norm": 6.753940582275391,
      "learning_rate": 2.4848435988390844e-05,
      "loss": 3.882,
      "step": 15800
    },
    {
      "epoch": 5.094520986863185,
      "grad_norm": 6.908844947814941,
      "learning_rate": 2.4687197678168335e-05,
      "loss": 3.7968,
      "step": 15900
    },
    {
      "epoch": 5.12656199935918,
      "grad_norm": 6.520855903625488,
      "learning_rate": 2.4525959367945827e-05,
      "loss": 3.8886,
      "step": 16000
    },
    {
      "epoch": 5.158603011855175,
      "grad_norm": 6.45058536529541,
      "learning_rate": 2.4364721057723318e-05,
      "loss": 3.8786,
      "step": 16100
    },
    {
      "epoch": 5.19064402435117,
      "grad_norm": 7.472925662994385,
      "learning_rate": 2.4203482747500806e-05,
      "loss": 3.9064,
      "step": 16200
    },
    {
      "epoch": 5.222685036847165,
      "grad_norm": 7.24752140045166,
      "learning_rate": 2.4042244437278298e-05,
      "loss": 3.8516,
      "step": 16300
    },
    {
      "epoch": 5.25472604934316,
      "grad_norm": 8.539881706237793,
      "learning_rate": 2.388100612705579e-05,
      "loss": 3.8667,
      "step": 16400
    },
    {
      "epoch": 5.2867670618391545,
      "grad_norm": 8.233210563659668,
      "learning_rate": 2.371976781683328e-05,
      "loss": 3.8445,
      "step": 16500
    },
    {
      "epoch": 5.318808074335149,
      "grad_norm": 8.435638427734375,
      "learning_rate": 2.3558529506610773e-05,
      "loss": 3.8317,
      "step": 16600
    },
    {
      "epoch": 5.350849086831144,
      "grad_norm": 6.60705041885376,
      "learning_rate": 2.3397291196388264e-05,
      "loss": 3.8766,
      "step": 16700
    },
    {
      "epoch": 5.382890099327139,
      "grad_norm": 6.440234661102295,
      "learning_rate": 2.3236052886165756e-05,
      "loss": 3.8939,
      "step": 16800
    },
    {
      "epoch": 5.414931111823134,
      "grad_norm": 8.18161678314209,
      "learning_rate": 2.3074814575943247e-05,
      "loss": 3.9453,
      "step": 16900
    },
    {
      "epoch": 5.446972124319128,
      "grad_norm": 6.707180976867676,
      "learning_rate": 2.291357626572074e-05,
      "loss": 3.8581,
      "step": 17000
    },
    {
      "epoch": 5.479013136815123,
      "grad_norm": 7.493718147277832,
      "learning_rate": 2.2752337955498227e-05,
      "loss": 3.8719,
      "step": 17100
    },
    {
      "epoch": 5.511054149311118,
      "grad_norm": 7.158884525299072,
      "learning_rate": 2.259109964527572e-05,
      "loss": 3.9038,
      "step": 17200
    },
    {
      "epoch": 5.543095161807113,
      "grad_norm": 6.918278217315674,
      "learning_rate": 2.242986133505321e-05,
      "loss": 3.9278,
      "step": 17300
    },
    {
      "epoch": 5.575136174303108,
      "grad_norm": 6.408534526824951,
      "learning_rate": 2.22686230248307e-05,
      "loss": 3.83,
      "step": 17400
    },
    {
      "epoch": 5.6071771867991025,
      "grad_norm": 8.166766166687012,
      "learning_rate": 2.2107384714608193e-05,
      "loss": 3.9154,
      "step": 17500
    },
    {
      "epoch": 5.639218199295097,
      "grad_norm": 8.53991413116455,
      "learning_rate": 2.1946146404385685e-05,
      "loss": 3.8767,
      "step": 17600
    },
    {
      "epoch": 5.671259211791092,
      "grad_norm": 8.675442695617676,
      "learning_rate": 2.1784908094163176e-05,
      "loss": 3.8748,
      "step": 17700
    },
    {
      "epoch": 5.703300224287087,
      "grad_norm": 6.764717102050781,
      "learning_rate": 2.1623669783940664e-05,
      "loss": 3.8721,
      "step": 17800
    },
    {
      "epoch": 5.735341236783082,
      "grad_norm": 7.175854682922363,
      "learning_rate": 2.1462431473718156e-05,
      "loss": 3.8673,
      "step": 17900
    },
    {
      "epoch": 5.767382249279077,
      "grad_norm": 6.636670112609863,
      "learning_rate": 2.1301193163495647e-05,
      "loss": 3.8698,
      "step": 18000
    },
    {
      "epoch": 5.799423261775072,
      "grad_norm": 6.170273780822754,
      "learning_rate": 2.113995485327314e-05,
      "loss": 3.8868,
      "step": 18100
    },
    {
      "epoch": 5.831464274271067,
      "grad_norm": 7.190518856048584,
      "learning_rate": 2.097871654305063e-05,
      "loss": 3.9031,
      "step": 18200
    },
    {
      "epoch": 5.863505286767062,
      "grad_norm": 5.571191787719727,
      "learning_rate": 2.0817478232828122e-05,
      "loss": 3.882,
      "step": 18300
    },
    {
      "epoch": 5.8955462992630565,
      "grad_norm": 8.593881607055664,
      "learning_rate": 2.065623992260561e-05,
      "loss": 3.9301,
      "step": 18400
    },
    {
      "epoch": 5.927587311759051,
      "grad_norm": 5.991754055023193,
      "learning_rate": 2.0495001612383102e-05,
      "loss": 3.9093,
      "step": 18500
    },
    {
      "epoch": 5.959628324255046,
      "grad_norm": 6.39769172668457,
      "learning_rate": 2.0333763302160593e-05,
      "loss": 3.9017,
      "step": 18600
    },
    {
      "epoch": 5.991669336751041,
      "grad_norm": 8.166033744812012,
      "learning_rate": 2.0172524991938085e-05,
      "loss": 3.8893,
      "step": 18700
    },
    {
      "epoch": 6.023710349247036,
      "grad_norm": 5.806833267211914,
      "learning_rate": 2.0011286681715576e-05,
      "loss": 3.7587,
      "step": 18800
    },
    {
      "epoch": 6.055751361743031,
      "grad_norm": 6.760810852050781,
      "learning_rate": 1.9850048371493068e-05,
      "loss": 3.7427,
      "step": 18900
    },
    {
      "epoch": 6.087792374239026,
      "grad_norm": 5.456637382507324,
      "learning_rate": 1.968881006127056e-05,
      "loss": 3.7748,
      "step": 19000
    },
    {
      "epoch": 6.119833386735021,
      "grad_norm": 8.947467803955078,
      "learning_rate": 1.9527571751048048e-05,
      "loss": 3.8114,
      "step": 19100
    },
    {
      "epoch": 6.151874399231016,
      "grad_norm": 9.621832847595215,
      "learning_rate": 1.936633344082554e-05,
      "loss": 3.7699,
      "step": 19200
    },
    {
      "epoch": 6.1839154117270105,
      "grad_norm": 6.204245567321777,
      "learning_rate": 1.920509513060303e-05,
      "loss": 3.7647,
      "step": 19300
    },
    {
      "epoch": 6.215956424223005,
      "grad_norm": 8.560230255126953,
      "learning_rate": 1.9043856820380522e-05,
      "loss": 3.7827,
      "step": 19400
    },
    {
      "epoch": 6.247997436719,
      "grad_norm": 7.79339075088501,
      "learning_rate": 1.8882618510158014e-05,
      "loss": 3.7329,
      "step": 19500
    },
    {
      "epoch": 6.280038449214995,
      "grad_norm": 7.622368335723877,
      "learning_rate": 1.8721380199935505e-05,
      "loss": 3.7567,
      "step": 19600
    },
    {
      "epoch": 6.31207946171099,
      "grad_norm": 7.433167934417725,
      "learning_rate": 1.8560141889712997e-05,
      "loss": 3.7925,
      "step": 19700
    },
    {
      "epoch": 6.344120474206985,
      "grad_norm": 8.560945510864258,
      "learning_rate": 1.839890357949049e-05,
      "loss": 3.7463,
      "step": 19800
    },
    {
      "epoch": 6.37616148670298,
      "grad_norm": 7.446340084075928,
      "learning_rate": 1.8237665269267977e-05,
      "loss": 3.7226,
      "step": 19900
    },
    {
      "epoch": 6.408202499198975,
      "grad_norm": 8.287322998046875,
      "learning_rate": 1.8076426959045468e-05,
      "loss": 3.7763,
      "step": 20000
    },
    {
      "epoch": 6.44024351169497,
      "grad_norm": 9.553872108459473,
      "learning_rate": 1.791518864882296e-05,
      "loss": 3.7592,
      "step": 20100
    },
    {
      "epoch": 6.472284524190965,
      "grad_norm": 6.430099964141846,
      "learning_rate": 1.775395033860045e-05,
      "loss": 3.7671,
      "step": 20200
    },
    {
      "epoch": 6.5043255366869595,
      "grad_norm": 7.725257396697998,
      "learning_rate": 1.7592712028377943e-05,
      "loss": 3.7532,
      "step": 20300
    },
    {
      "epoch": 6.536366549182954,
      "grad_norm": 7.6287312507629395,
      "learning_rate": 1.7431473718155434e-05,
      "loss": 3.8021,
      "step": 20400
    },
    {
      "epoch": 6.568407561678949,
      "grad_norm": 5.747056007385254,
      "learning_rate": 1.7270235407932926e-05,
      "loss": 3.7427,
      "step": 20500
    },
    {
      "epoch": 6.600448574174944,
      "grad_norm": 6.658758163452148,
      "learning_rate": 1.7108997097710418e-05,
      "loss": 3.803,
      "step": 20600
    },
    {
      "epoch": 6.632489586670939,
      "grad_norm": 7.563108444213867,
      "learning_rate": 1.694775878748791e-05,
      "loss": 3.7478,
      "step": 20700
    },
    {
      "epoch": 6.664530599166934,
      "grad_norm": 7.893658638000488,
      "learning_rate": 1.67865204772654e-05,
      "loss": 3.7697,
      "step": 20800
    },
    {
      "epoch": 6.696571611662929,
      "grad_norm": 7.055954933166504,
      "learning_rate": 1.6625282167042892e-05,
      "loss": 3.8217,
      "step": 20900
    },
    {
      "epoch": 6.728612624158924,
      "grad_norm": 8.379875183105469,
      "learning_rate": 1.6464043856820384e-05,
      "loss": 3.7946,
      "step": 21000
    },
    {
      "epoch": 6.760653636654919,
      "grad_norm": 7.4173994064331055,
      "learning_rate": 1.6302805546597872e-05,
      "loss": 3.7542,
      "step": 21100
    },
    {
      "epoch": 6.7926946491509135,
      "grad_norm": 7.620007514953613,
      "learning_rate": 1.6141567236375363e-05,
      "loss": 3.7683,
      "step": 21200
    },
    {
      "epoch": 6.824735661646908,
      "grad_norm": 8.471660614013672,
      "learning_rate": 1.5980328926152855e-05,
      "loss": 3.8009,
      "step": 21300
    },
    {
      "epoch": 6.856776674142903,
      "grad_norm": 6.1167755126953125,
      "learning_rate": 1.5819090615930347e-05,
      "loss": 3.8038,
      "step": 21400
    },
    {
      "epoch": 6.888817686638898,
      "grad_norm": 6.205162048339844,
      "learning_rate": 1.5657852305707838e-05,
      "loss": 3.7868,
      "step": 21500
    },
    {
      "epoch": 6.920858699134893,
      "grad_norm": 7.556000232696533,
      "learning_rate": 1.549661399548533e-05,
      "loss": 3.807,
      "step": 21600
    },
    {
      "epoch": 6.952899711630888,
      "grad_norm": 8.199142456054688,
      "learning_rate": 1.533537568526282e-05,
      "loss": 3.7565,
      "step": 21700
    },
    {
      "epoch": 6.984940724126883,
      "grad_norm": 8.190046310424805,
      "learning_rate": 1.517413737504031e-05,
      "loss": 3.771,
      "step": 21800
    },
    {
      "epoch": 7.016981736622878,
      "grad_norm": 6.03689432144165,
      "learning_rate": 1.5012899064817801e-05,
      "loss": 3.7237,
      "step": 21900
    },
    {
      "epoch": 7.049022749118872,
      "grad_norm": 7.495515823364258,
      "learning_rate": 1.4851660754595292e-05,
      "loss": 3.6786,
      "step": 22000
    },
    {
      "epoch": 7.081063761614867,
      "grad_norm": 9.693270683288574,
      "learning_rate": 1.4690422444372784e-05,
      "loss": 3.6516,
      "step": 22100
    },
    {
      "epoch": 7.1131047741108615,
      "grad_norm": 7.584794521331787,
      "learning_rate": 1.4529184134150275e-05,
      "loss": 3.6805,
      "step": 22200
    },
    {
      "epoch": 7.145145786606856,
      "grad_norm": 7.965902805328369,
      "learning_rate": 1.4367945823927767e-05,
      "loss": 3.6621,
      "step": 22300
    },
    {
      "epoch": 7.177186799102851,
      "grad_norm": 7.572253227233887,
      "learning_rate": 1.4206707513705255e-05,
      "loss": 3.6616,
      "step": 22400
    },
    {
      "epoch": 7.209227811598846,
      "grad_norm": 7.331798553466797,
      "learning_rate": 1.4045469203482747e-05,
      "loss": 3.6818,
      "step": 22500
    },
    {
      "epoch": 7.241268824094841,
      "grad_norm": 8.050657272338867,
      "learning_rate": 1.3884230893260238e-05,
      "loss": 3.6455,
      "step": 22600
    },
    {
      "epoch": 7.273309836590836,
      "grad_norm": 6.594356536865234,
      "learning_rate": 1.372299258303773e-05,
      "loss": 3.6507,
      "step": 22700
    },
    {
      "epoch": 7.305350849086831,
      "grad_norm": 6.399558067321777,
      "learning_rate": 1.3561754272815221e-05,
      "loss": 3.7082,
      "step": 22800
    },
    {
      "epoch": 7.337391861582826,
      "grad_norm": 8.258371353149414,
      "learning_rate": 1.3400515962592713e-05,
      "loss": 3.6882,
      "step": 22900
    },
    {
      "epoch": 7.369432874078821,
      "grad_norm": 6.693822860717773,
      "learning_rate": 1.3239277652370204e-05,
      "loss": 3.7202,
      "step": 23000
    },
    {
      "epoch": 7.4014738865748155,
      "grad_norm": 6.96484899520874,
      "learning_rate": 1.3078039342147694e-05,
      "loss": 3.7156,
      "step": 23100
    },
    {
      "epoch": 7.43351489907081,
      "grad_norm": 7.9764404296875,
      "learning_rate": 1.2916801031925186e-05,
      "loss": 3.6546,
      "step": 23200
    },
    {
      "epoch": 7.465555911566805,
      "grad_norm": 7.311800956726074,
      "learning_rate": 1.2755562721702677e-05,
      "loss": 3.6647,
      "step": 23300
    },
    {
      "epoch": 7.4975969240628,
      "grad_norm": 7.817023277282715,
      "learning_rate": 1.2594324411480169e-05,
      "loss": 3.6509,
      "step": 23400
    },
    {
      "epoch": 7.529637936558795,
      "grad_norm": 8.473406791687012,
      "learning_rate": 1.243308610125766e-05,
      "loss": 3.6615,
      "step": 23500
    },
    {
      "epoch": 7.56167894905479,
      "grad_norm": 7.711569309234619,
      "learning_rate": 1.227184779103515e-05,
      "loss": 3.6985,
      "step": 23600
    },
    {
      "epoch": 7.593719961550785,
      "grad_norm": 9.169897079467773,
      "learning_rate": 1.2110609480812642e-05,
      "loss": 3.6313,
      "step": 23700
    },
    {
      "epoch": 7.62576097404678,
      "grad_norm": 8.442765235900879,
      "learning_rate": 1.1949371170590133e-05,
      "loss": 3.6797,
      "step": 23800
    },
    {
      "epoch": 7.657801986542775,
      "grad_norm": 8.472480773925781,
      "learning_rate": 1.1788132860367623e-05,
      "loss": 3.6737,
      "step": 23900
    },
    {
      "epoch": 7.6898429990387696,
      "grad_norm": 8.425004005432129,
      "learning_rate": 1.1626894550145115e-05,
      "loss": 3.7518,
      "step": 24000
    },
    {
      "epoch": 7.7218840115347644,
      "grad_norm": 7.4330596923828125,
      "learning_rate": 1.1465656239922606e-05,
      "loss": 3.7279,
      "step": 24100
    },
    {
      "epoch": 7.753925024030759,
      "grad_norm": 6.998435974121094,
      "learning_rate": 1.1304417929700098e-05,
      "loss": 3.6872,
      "step": 24200
    },
    {
      "epoch": 7.785966036526754,
      "grad_norm": 6.652815818786621,
      "learning_rate": 1.1143179619477588e-05,
      "loss": 3.6917,
      "step": 24300
    },
    {
      "epoch": 7.818007049022749,
      "grad_norm": 8.868033409118652,
      "learning_rate": 1.098194130925508e-05,
      "loss": 3.6429,
      "step": 24400
    },
    {
      "epoch": 7.850048061518744,
      "grad_norm": 6.141758918762207,
      "learning_rate": 1.0820702999032571e-05,
      "loss": 3.7208,
      "step": 24500
    },
    {
      "epoch": 7.882089074014739,
      "grad_norm": 9.037554740905762,
      "learning_rate": 1.065946468881006e-05,
      "loss": 3.6609,
      "step": 24600
    },
    {
      "epoch": 7.914130086510734,
      "grad_norm": 6.661378383636475,
      "learning_rate": 1.0498226378587552e-05,
      "loss": 3.6654,
      "step": 24700
    },
    {
      "epoch": 7.946171099006729,
      "grad_norm": 7.5912017822265625,
      "learning_rate": 1.0336988068365044e-05,
      "loss": 3.6635,
      "step": 24800
    },
    {
      "epoch": 7.978212111502724,
      "grad_norm": 7.149954319000244,
      "learning_rate": 1.0175749758142534e-05,
      "loss": 3.727,
      "step": 24900
    },
    {
      "epoch": 8.010253123998718,
      "grad_norm": 7.146185398101807,
      "learning_rate": 1.0014511447920025e-05,
      "loss": 3.6784,
      "step": 25000
    },
    {
      "epoch": 8.042294136494712,
      "grad_norm": 9.25921630859375,
      "learning_rate": 9.853273137697517e-06,
      "loss": 3.5975,
      "step": 25100
    },
    {
      "epoch": 8.074335148990707,
      "grad_norm": 8.900440216064453,
      "learning_rate": 9.692034827475008e-06,
      "loss": 3.6036,
      "step": 25200
    },
    {
      "epoch": 8.106376161486702,
      "grad_norm": 9.210308074951172,
      "learning_rate": 9.5307965172525e-06,
      "loss": 3.578,
      "step": 25300
    },
    {
      "epoch": 8.138417173982697,
      "grad_norm": 8.123282432556152,
      "learning_rate": 9.369558207029991e-06,
      "loss": 3.5821,
      "step": 25400
    },
    {
      "epoch": 8.170458186478692,
      "grad_norm": 9.054661750793457,
      "learning_rate": 9.208319896807483e-06,
      "loss": 3.6179,
      "step": 25500
    },
    {
      "epoch": 8.202499198974687,
      "grad_norm": 8.016879081726074,
      "learning_rate": 9.047081586584973e-06,
      "loss": 3.605,
      "step": 25600
    },
    {
      "epoch": 8.234540211470682,
      "grad_norm": 9.267966270446777,
      "learning_rate": 8.885843276362464e-06,
      "loss": 3.6386,
      "step": 25700
    },
    {
      "epoch": 8.266581223966677,
      "grad_norm": 8.02328872680664,
      "learning_rate": 8.724604966139956e-06,
      "loss": 3.596,
      "step": 25800
    },
    {
      "epoch": 8.298622236462672,
      "grad_norm": 8.872678756713867,
      "learning_rate": 8.563366655917446e-06,
      "loss": 3.5856,
      "step": 25900
    },
    {
      "epoch": 8.330663248958667,
      "grad_norm": 9.257118225097656,
      "learning_rate": 8.402128345694937e-06,
      "loss": 3.6181,
      "step": 26000
    },
    {
      "epoch": 8.362704261454661,
      "grad_norm": 7.016186237335205,
      "learning_rate": 8.240890035472429e-06,
      "loss": 3.6045,
      "step": 26100
    },
    {
      "epoch": 8.394745273950656,
      "grad_norm": 7.977199554443359,
      "learning_rate": 8.07965172524992e-06,
      "loss": 3.5977,
      "step": 26200
    },
    {
      "epoch": 8.426786286446651,
      "grad_norm": 7.537696838378906,
      "learning_rate": 7.91841341502741e-06,
      "loss": 3.6403,
      "step": 26300
    },
    {
      "epoch": 8.458827298942646,
      "grad_norm": 6.593136787414551,
      "learning_rate": 7.757175104804902e-06,
      "loss": 3.6136,
      "step": 26400
    },
    {
      "epoch": 8.490868311438641,
      "grad_norm": 10.085128784179688,
      "learning_rate": 7.5959367945823934e-06,
      "loss": 3.6002,
      "step": 26500
    },
    {
      "epoch": 8.522909323934636,
      "grad_norm": 10.167741775512695,
      "learning_rate": 7.434698484359884e-06,
      "loss": 3.6417,
      "step": 26600
    },
    {
      "epoch": 8.55495033643063,
      "grad_norm": 8.358238220214844,
      "learning_rate": 7.273460174137376e-06,
      "loss": 3.6306,
      "step": 26700
    },
    {
      "epoch": 8.586991348926626,
      "grad_norm": 11.044875144958496,
      "learning_rate": 7.112221863914867e-06,
      "loss": 3.6493,
      "step": 26800
    },
    {
      "epoch": 8.61903236142262,
      "grad_norm": 7.625738620758057,
      "learning_rate": 6.950983553692357e-06,
      "loss": 3.6205,
      "step": 26900
    },
    {
      "epoch": 8.651073373918615,
      "grad_norm": 8.38176155090332,
      "learning_rate": 6.789745243469849e-06,
      "loss": 3.5926,
      "step": 27000
    },
    {
      "epoch": 8.68311438641461,
      "grad_norm": 7.454788684844971,
      "learning_rate": 6.62850693324734e-06,
      "loss": 3.5925,
      "step": 27100
    },
    {
      "epoch": 8.715155398910605,
      "grad_norm": 8.292949676513672,
      "learning_rate": 6.467268623024832e-06,
      "loss": 3.6322,
      "step": 27200
    },
    {
      "epoch": 8.7471964114066,
      "grad_norm": 9.590886116027832,
      "learning_rate": 6.306030312802322e-06,
      "loss": 3.5987,
      "step": 27300
    },
    {
      "epoch": 8.779237423902595,
      "grad_norm": 7.145565986633301,
      "learning_rate": 6.144792002579813e-06,
      "loss": 3.6587,
      "step": 27400
    },
    {
      "epoch": 8.81127843639859,
      "grad_norm": 7.404107093811035,
      "learning_rate": 5.983553692357304e-06,
      "loss": 3.5942,
      "step": 27500
    },
    {
      "epoch": 8.843319448894585,
      "grad_norm": 6.718050003051758,
      "learning_rate": 5.822315382134795e-06,
      "loss": 3.6052,
      "step": 27600
    },
    {
      "epoch": 8.87536046139058,
      "grad_norm": 8.663691520690918,
      "learning_rate": 5.661077071912287e-06,
      "loss": 3.5922,
      "step": 27700
    },
    {
      "epoch": 8.907401473886575,
      "grad_norm": 6.780577659606934,
      "learning_rate": 5.499838761689778e-06,
      "loss": 3.5874,
      "step": 27800
    },
    {
      "epoch": 8.93944248638257,
      "grad_norm": 8.152008056640625,
      "learning_rate": 5.338600451467269e-06,
      "loss": 3.6045,
      "step": 27900
    },
    {
      "epoch": 8.971483498878564,
      "grad_norm": 6.872178077697754,
      "learning_rate": 5.17736214124476e-06,
      "loss": 3.5804,
      "step": 28000
    },
    {
      "epoch": 9.00352451137456,
      "grad_norm": 9.140951156616211,
      "learning_rate": 5.0161238310222514e-06,
      "loss": 3.6509,
      "step": 28100
    },
    {
      "epoch": 9.035565523870554,
      "grad_norm": 7.0498785972595215,
      "learning_rate": 4.854885520799742e-06,
      "loss": 3.6195,
      "step": 28200
    },
    {
      "epoch": 9.067606536366549,
      "grad_norm": 7.244235038757324,
      "learning_rate": 4.693647210577233e-06,
      "loss": 3.5631,
      "step": 28300
    },
    {
      "epoch": 9.099647548862544,
      "grad_norm": 9.666976928710938,
      "learning_rate": 4.532408900354724e-06,
      "loss": 3.5567,
      "step": 28400
    },
    {
      "epoch": 9.131688561358539,
      "grad_norm": 9.948286056518555,
      "learning_rate": 4.371170590132215e-06,
      "loss": 3.5407,
      "step": 28500
    },
    {
      "epoch": 9.163729573854534,
      "grad_norm": 7.88555383682251,
      "learning_rate": 4.209932279909707e-06,
      "loss": 3.5524,
      "step": 28600
    },
    {
      "epoch": 9.195770586350529,
      "grad_norm": 8.41462516784668,
      "learning_rate": 4.048693969687198e-06,
      "loss": 3.565,
      "step": 28700
    },
    {
      "epoch": 9.227811598846523,
      "grad_norm": 7.033403396606445,
      "learning_rate": 3.887455659464689e-06,
      "loss": 3.5349,
      "step": 28800
    },
    {
      "epoch": 9.259852611342518,
      "grad_norm": 8.220091819763184,
      "learning_rate": 3.7262173492421804e-06,
      "loss": 3.5268,
      "step": 28900
    },
    {
      "epoch": 9.291893623838513,
      "grad_norm": 9.788064956665039,
      "learning_rate": 3.564979039019671e-06,
      "loss": 3.5376,
      "step": 29000
    },
    {
      "epoch": 9.323934636334508,
      "grad_norm": 7.10802698135376,
      "learning_rate": 3.4037407287971627e-06,
      "loss": 3.5377,
      "step": 29100
    },
    {
      "epoch": 9.355975648830503,
      "grad_norm": 7.954392910003662,
      "learning_rate": 3.2425024185746534e-06,
      "loss": 3.5405,
      "step": 29200
    },
    {
      "epoch": 9.388016661326498,
      "grad_norm": 8.038957595825195,
      "learning_rate": 3.0812641083521445e-06,
      "loss": 3.5139,
      "step": 29300
    },
    {
      "epoch": 9.420057673822493,
      "grad_norm": 8.701301574707031,
      "learning_rate": 2.920025798129636e-06,
      "loss": 3.5653,
      "step": 29400
    },
    {
      "epoch": 9.452098686318488,
      "grad_norm": 7.469971656799316,
      "learning_rate": 2.758787487907127e-06,
      "loss": 3.5208,
      "step": 29500
    },
    {
      "epoch": 9.484139698814483,
      "grad_norm": 8.240509986877441,
      "learning_rate": 2.597549177684618e-06,
      "loss": 3.5701,
      "step": 29600
    },
    {
      "epoch": 9.516180711310477,
      "grad_norm": 9.762605667114258,
      "learning_rate": 2.436310867462109e-06,
      "loss": 3.6149,
      "step": 29700
    },
    {
      "epoch": 9.548221723806472,
      "grad_norm": 7.3648457527160645,
      "learning_rate": 2.2750725572396e-06,
      "loss": 3.5219,
      "step": 29800
    },
    {
      "epoch": 9.580262736302467,
      "grad_norm": 7.9283013343811035,
      "learning_rate": 2.1138342470170917e-06,
      "loss": 3.597,
      "step": 29900
    },
    {
      "epoch": 9.612303748798462,
      "grad_norm": 9.072404861450195,
      "learning_rate": 1.952595936794583e-06,
      "loss": 3.5568,
      "step": 30000
    },
    {
      "epoch": 9.644344761294457,
      "grad_norm": 7.002293586730957,
      "learning_rate": 1.7913576265720735e-06,
      "loss": 3.5416,
      "step": 30100
    },
    {
      "epoch": 9.676385773790452,
      "grad_norm": 8.024081230163574,
      "learning_rate": 1.6301193163495646e-06,
      "loss": 3.6072,
      "step": 30200
    },
    {
      "epoch": 9.708426786286447,
      "grad_norm": 6.8877272605896,
      "learning_rate": 1.468881006127056e-06,
      "loss": 3.5918,
      "step": 30300
    },
    {
      "epoch": 9.740467798782442,
      "grad_norm": 7.472293853759766,
      "learning_rate": 1.3076426959045469e-06,
      "loss": 3.5572,
      "step": 30400
    },
    {
      "epoch": 9.772508811278437,
      "grad_norm": 9.983868598937988,
      "learning_rate": 1.1464043856820382e-06,
      "loss": 3.5422,
      "step": 30500
    },
    {
      "epoch": 9.804549823774432,
      "grad_norm": 7.573673248291016,
      "learning_rate": 9.851660754595293e-07,
      "loss": 3.5824,
      "step": 30600
    },
    {
      "epoch": 9.836590836270426,
      "grad_norm": 9.56699275970459,
      "learning_rate": 8.239277652370203e-07,
      "loss": 3.5562,
      "step": 30700
    },
    {
      "epoch": 9.868631848766421,
      "grad_norm": 8.115558624267578,
      "learning_rate": 6.626894550145115e-07,
      "loss": 3.5624,
      "step": 30800
    },
    {
      "epoch": 9.900672861262416,
      "grad_norm": 7.845049858093262,
      "learning_rate": 5.014511447920026e-07,
      "loss": 3.5476,
      "step": 30900
    },
    {
      "epoch": 9.932713873758411,
      "grad_norm": 8.629937171936035,
      "learning_rate": 3.402128345694937e-07,
      "loss": 3.6186,
      "step": 31000
    }
  ],
  "logging_steps": 100,
  "max_steps": 31210,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5642577955584000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
